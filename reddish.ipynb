{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reddish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JelenaMitrovic/Jelena/blob/master/reddish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xioN-O_vtztC",
        "colab_type": "code",
        "outputId": "8e830592-afa7-4861-eb3a-53fd6bd0be4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title Download Data\n",
        "from google.colab import drive, auth\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doNFRjPqiBhM",
        "colab_type": "code",
        "outputId": "f35ce264-4c6d-4999-83e1-19a2fe642ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "# @title Preparation\n",
        "!pip install -q keras-bert keras-rectified-adam\n",
        "#!pip install -q keras-bert-tpu keras-rectified-adam\n",
        "!pip install tensorflow==1.5\n",
        "\n",
        "import os\n",
        "from random import seed\n",
        "import numpy as np\n",
        "\n",
        "from keras_bert.datasets import get_pretrained, PretrainedList\n",
        "#model_path = get_pretrained(PretrainedList.flaubert_base_uncased)\n",
        "print (PretrainedList.__dict__)\n",
        "\n",
        "SEQ_LEN = 100\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5\n",
        "LR = 1e-5\n",
        "\n",
        "\n",
        "bert_model = {\n",
        "        'model': 'uncased_L-12_H-768_A-12',\n",
        "        'config': 'bert_config.json',\n",
        "        'checkpoint': 'bert_model.ckpt',\n",
        "        'vocab': 'vocab.txt'\n",
        "}    \n",
        "\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/bert-models/'\n",
        "pretrained_path = os.path.join(data_path, bert_model['model'])\n",
        "config_path = os.path.join(pretrained_path, bert_model['config'])\n",
        "checkpoint_path = os.path.join(pretrained_path, bert_model['checkpoint'])\n",
        "vocab_path = os.path.join(pretrained_path, bert_model['vocab'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/79/a37d0b373757b4d283c674a64127bd8864d69f881c639b1ee5953e2d9301/tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (44.4MB)\n",
            "\u001b[K     |████████████████████████████████| 44.4MB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.12.0)\n",
            "Collecting tensorflow-tensorboard<1.6.0,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/fa/91c06952517b4f1bc075545b062a4112e30cebe558a6b962816cb33efa27/tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.18.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.34.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.2.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.0.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.5) (46.0.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=0db033903b28aa94768f3fd9e1d3ffe1a2a71a05d2ad591cf1e024347ae04cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.3\n",
            "    Uninstalling bleach-3.1.3:\n",
            "      Successfully uninstalled bleach-3.1.3\n",
            "  Found existing installation: tensorflow 2.2.0rc1\n",
            "    Uninstalling tensorflow-2.2.0rc1:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc1\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'__module__': 'keras_bert.datasets.pretrained', '__test__': PretrainedInfo(url='https://github.com/CyberZHG/keras-bert/archive/master.zip', extract_name='keras-bert-master', target_name='keras-bert'), 'multi_cased_base': 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip', 'chinese_base': 'https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip', 'wwm_uncased_large': 'https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip', 'wwm_cased_large': 'https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip', 'chinese_wwm_base': PretrainedInfo(url='https://storage.googleapis.com/hfl-rc/chinese-bert/chinese_wwm_L-12_H-768_A-12.zip', extract_name='publish', target_name='chinese_wwm_L-12_H-768_A-12'), '__dict__': <attribute '__dict__' of 'PretrainedList' objects>, '__weakref__': <attribute '__weakref__' of 'PretrainedList' objects>, '__doc__': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVTPNxOyj4HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Load Basic Model\n",
        "import codecs\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=True,\n",
        "    trainable=True,\n",
        "    seq_len=SEQ_LEN,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfC3Nh8pnckd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras_bert import Tokenizer\n",
        "import pickle\n",
        "import csv\n",
        "\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "def load_data(path, delimiter=\",\", fields=['id', 'text', 'label'], transform={}):\n",
        "    global tokenizer\n",
        "    indices, sentiments = [], []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f, delimiter=delimiter)\n",
        "        for row in reader:\n",
        "            ids, segments = tokenizer.encode(row[fields[1]], max_len=SEQ_LEN)\n",
        "            indices.append(ids)\n",
        "            sentiments.append(transform[row[fields[2]]])\n",
        "\n",
        "    items = list(zip(indices, sentiments))\n",
        "    np.random.shuffle(items)\n",
        "    indices, sentiments = zip(*items)\n",
        "    indices = np.array(indices)\n",
        "    mod = indices.shape[0] % BATCH_SIZE\n",
        "    if mod > 0:\n",
        "        indices, sentiments = indices[:-mod], sentiments[:-mod]\n",
        "    return [indices, np.zeros_like(indices)], np.array(sentiments)\n",
        "  \n",
        "base_path = '/content/drive/My Drive/Colab Notebooks/data/'\n",
        "\n",
        "# below:\n",
        "# transform={\"NOTABU\":0, \"IMP\":1, \"EXP\":1} for binary abusive classification\n",
        "# transform={\"NOTABU\":0, \"IMP\":1, \"EXP\":2} for three-way classification\n",
        "\n",
        "# use this for the baseline\n",
        "#train_path = os.path.join(base_path, 'abuseval/train.csv')\n",
        "#train_x, train_y = load_data(train_path, delimiter=\"\\t\", fields=['id', 'text', 'abuse'], transform={\"NOTABU\":0, \"IMP\":1, \"EXP\":2}) \n",
        "\n",
        "# use this for the distant supervision data only\n",
        "#train_path = os.path.join(base_path, 'reddish/reddish.resampled33-33-33.10000.csv')\n",
        "#train_x, train_y = load_data(train_path, fields=['comment_id', 'text', 'abusive'], transform={\"NOT\":0, \"IMP\":1, \"EXP\":2})\n",
        "\n",
        "# use this for the distant supervision data in addition to AbusEval\n",
        "train_path = os.path.join(base_path, 'reddish/abuseval+reddish.resampled33-33-33.10000.csv')\n",
        "train_x, train_y = load_data(train_path, fields=['id', 'text', 'abuse'], transform={\"NOTABU\":0, \"IMP\":1, \"EXP\":2})\n",
        "\n",
        "test_path = os.path.join(base_path, 'abuseval/test.csv')\n",
        "test_x, test_y = load_data(test_path, delimiter=\"\\t\", fields=['id', 'text', 'abuse'], transform={\"NOTABU\":0, \"IMP\":1, \"EXP\":2})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhMA1j7wnqSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Build Custom Model\n",
        "import keras\n",
        "from keras_radam import RAdam\n",
        "\n",
        "inputs = model.inputs[:2]\n",
        "dense = model.get_layer('NSP-Dense').output\n",
        "\n",
        "# change units={2|3} depending on the classification task\n",
        "outputs = keras.layers.Dense(units=3, activation='softmax')(dense)\n",
        "\n",
        "model = keras.models.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    RAdam(lr=LR),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy'],\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgP7bCQxrZpQ",
        "colab_type": "code",
        "outputId": "9a1bedfc-9778-47b2-89f1-0f4173bd6646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# @title Fit\n",
        "\n",
        "model.fit(\n",
        "    train_x,\n",
        "    train_y,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  344/23112 [..............................] - ETA: 26:01:27 - loss: 1.1613 - sparse_categorical_accuracy: 0.2616"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBSba3vprlRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Predict\n",
        "predicts = model.predict(test_x, verbose=True).argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo1aps8prrCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Accuracy\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "\n",
        "accuracy = (np.sum(test_y == predicts) / test_y.shape[0])\n",
        "print (confusion_matrix(test_y, predicts))\n",
        "report = classification_report(test_y, predicts, output_dict=True)\n",
        "df = pd.DataFrame(report)\n",
        "print(df)\n",
        "\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for c in [0,1]:\n",
        "    for g, p in zip(test_y, predicts):\n",
        "        if g == p: \n",
        "            if p==c:\n",
        "                tp+=1\n",
        "            else:\n",
        "                tn+=1\n",
        "        else:\n",
        "            if p==c: \n",
        "                fp+=1\n",
        "            else: \n",
        "                fn+=1\n",
        "microp = tp/(tp+fp)\n",
        "micror = tp/(tp+fn)\n",
        "microf1 = (2*microp*micror)/(microp+micror)\n",
        "\n",
        "print (\"\\t\".join([\"{0:.3f}\".format(v) for v in [\n",
        "       df['0']['precision'],\n",
        "       df['0']['recall'],\n",
        "       df['0']['f1-score'],\n",
        "       df['1']['precision'],\n",
        "       df['1']['recall'],\n",
        "       df['1']['f1-score'],\n",
        "       df['2']['precision'],\n",
        "       df['2']['recall'],\n",
        "       df['2']['f1-score'],\n",
        "       df['macro avg']['precision'],\n",
        "       df['macro avg']['recall'],\n",
        "       df['macro avg']['f1-score'],\n",
        "       microp, \n",
        "       micror, \n",
        "       microf1,\n",
        "       accuracy]]).replace(\".\", \",\"))\n",
        "\n",
        "\n",
        "\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}